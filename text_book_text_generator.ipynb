{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ea3695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f07075c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of The Picture of Dorian Gray This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"Dataset/thepictureofdoriangray.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) \n",
    " \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('“','').replace('”','')\n",
    "\n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4a56ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 131, 261, 651, 2, 1, 157, 2, 27, 63, 62, 651, 14, 18, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    " \n",
    "# saving the tokenizer to predict \n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    " \n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26e7a8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7464\n",
      "The Length of sequences are:  82697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1, 131, 261, 651],\n",
       "       [131, 261, 651,   2],\n",
       "       [261, 651,   2,   1],\n",
       "       [651,   2,   1, 157],\n",
       "       [  2,   1, 157,   2],\n",
       "       [  1, 157,   2,  27],\n",
       "       [157,   2,  27,  63],\n",
       "       [  2,  27,  63,  62],\n",
       "       [ 27,  63,  62, 651],\n",
       "       [ 63,  62, 651,  14]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "sequences = []\n",
    " \n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "     \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c847efa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[  1 131 261]\n",
      " [131 261 651]\n",
      " [261 651   2]\n",
      " [651   2   1]\n",
      " [  2   1 157]\n",
      " [  1 157   2]\n",
      " [157   2  27]\n",
      " [  2  27  63]\n",
      " [ 27  63  62]\n",
      " [ 63  62 651]]\n",
      "Response:  [651   2   1 157   2  27  63  62 651  14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cd52aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 6.4800\n",
      "Epoch 1: loss improved from inf to 6.47999, saving model to next_words.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsh\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1293/1293 [==============================] - 326s 249ms/step - loss: 6.4800\n",
      "Epoch 2/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 5.9009\n",
      "Epoch 2: loss improved from 6.47999 to 5.90091, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 326s 252ms/step - loss: 5.9009\n",
      "Epoch 3/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 5.5262\n",
      "Epoch 3: loss improved from 5.90091 to 5.52617, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 321s 248ms/step - loss: 5.5262\n",
      "Epoch 4/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 5.2407\n",
      "Epoch 4: loss improved from 5.52617 to 5.24068, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 318s 246ms/step - loss: 5.2407\n",
      "Epoch 5/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 4.9991\n",
      "Epoch 5: loss improved from 5.24068 to 4.99907, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 4.9991\n",
      "Epoch 6/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 4.7693\n",
      "Epoch 6: loss improved from 4.99907 to 4.76928, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 4.7693\n",
      "Epoch 7/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 4.5345\n",
      "Epoch 7: loss improved from 4.76928 to 4.53454, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 325s 252ms/step - loss: 4.5345\n",
      "Epoch 8/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 4.2872\n",
      "Epoch 8: loss improved from 4.53454 to 4.28721, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 340s 263ms/step - loss: 4.2872\n",
      "Epoch 9/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 4.0342\n",
      "Epoch 9: loss improved from 4.28721 to 4.03420, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 339s 263ms/step - loss: 4.0342\n",
      "Epoch 10/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 3.7744\n",
      "Epoch 10: loss improved from 4.03420 to 3.77437, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 338s 261ms/step - loss: 3.7744\n",
      "Epoch 11/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 3.5157\n",
      "Epoch 11: loss improved from 3.77437 to 3.51565, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 336s 260ms/step - loss: 3.5157\n",
      "Epoch 12/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 3.2523\n",
      "Epoch 12: loss improved from 3.51565 to 3.25228, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 338s 261ms/step - loss: 3.2523\n",
      "Epoch 13/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 3.0062\n",
      "Epoch 13: loss improved from 3.25228 to 3.00617, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 337s 261ms/step - loss: 3.0062\n",
      "Epoch 14/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 2.7625\n",
      "Epoch 14: loss improved from 3.00617 to 2.76253, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 342s 265ms/step - loss: 2.7625\n",
      "Epoch 15/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 2.5270\n",
      "Epoch 15: loss improved from 2.76253 to 2.52699, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 340s 263ms/step - loss: 2.5270\n",
      "Epoch 16/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 2.2882\n",
      "Epoch 16: loss improved from 2.52699 to 2.28818, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 337s 261ms/step - loss: 2.2882\n",
      "Epoch 17/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 2.0587\n",
      "Epoch 17: loss improved from 2.28818 to 2.05869, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 324s 250ms/step - loss: 2.0587\n",
      "Epoch 18/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 1.8312\n",
      "Epoch 18: loss improved from 2.05869 to 1.83116, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 326s 252ms/step - loss: 1.8312\n",
      "Epoch 19/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 1.6221\n",
      "Epoch 19: loss improved from 1.83116 to 1.62208, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 331s 256ms/step - loss: 1.6221\n",
      "Epoch 20/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 1.4286\n",
      "Epoch 20: loss improved from 1.62208 to 1.42862, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 338s 262ms/step - loss: 1.4286\n",
      "Epoch 21/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 1.2562\n",
      "Epoch 21: loss improved from 1.42862 to 1.25621, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 325s 252ms/step - loss: 1.2562\n",
      "Epoch 22/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 1.1112\n",
      "Epoch 22: loss improved from 1.25621 to 1.11121, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 324s 250ms/step - loss: 1.1112\n",
      "Epoch 23/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.9954\n",
      "Epoch 23: loss improved from 1.11121 to 0.99542, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 317s 245ms/step - loss: 0.9954\n",
      "Epoch 24/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.8970\n",
      "Epoch 24: loss improved from 0.99542 to 0.89700, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 322s 249ms/step - loss: 0.8970\n",
      "Epoch 25/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.8201\n",
      "Epoch 25: loss improved from 0.89700 to 0.82015, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 0.8201\n",
      "Epoch 26/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.7621\n",
      "Epoch 26: loss improved from 0.82015 to 0.76210, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 319s 247ms/step - loss: 0.7621\n",
      "Epoch 27/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.7166\n",
      "Epoch 27: loss improved from 0.76210 to 0.71663, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 321s 248ms/step - loss: 0.7166\n",
      "Epoch 28/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.6740\n",
      "Epoch 28: loss improved from 0.71663 to 0.67400, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 322s 249ms/step - loss: 0.6740\n",
      "Epoch 29/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.6440\n",
      "Epoch 29: loss improved from 0.67400 to 0.64401, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 319s 247ms/step - loss: 0.6440\n",
      "Epoch 30/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.6161\n",
      "Epoch 30: loss improved from 0.64401 to 0.61607, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 324s 250ms/step - loss: 0.6161\n",
      "Epoch 31/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.5915\n",
      "Epoch 31: loss improved from 0.61607 to 0.59145, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 321s 248ms/step - loss: 0.5915\n",
      "Epoch 32/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.5755\n",
      "Epoch 32: loss improved from 0.59145 to 0.57553, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 320s 247ms/step - loss: 0.5755\n",
      "Epoch 33/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.5614\n",
      "Epoch 33: loss improved from 0.57553 to 0.56140, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 321s 248ms/step - loss: 0.5614\n",
      "Epoch 34/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.5421\n",
      "Epoch 34: loss improved from 0.56140 to 0.54207, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 322s 249ms/step - loss: 0.5421\n",
      "Epoch 35/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.5318\n",
      "Epoch 35: loss improved from 0.54207 to 0.53181, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 322s 249ms/step - loss: 0.5318\n",
      "Epoch 36/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.5130\n",
      "Epoch 36: loss improved from 0.53181 to 0.51302, saving model to next_words.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1293/1293 [==============================] - 327s 253ms/step - loss: 0.5130\n",
      "Epoch 37/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.5046\n",
      "Epoch 37: loss improved from 0.51302 to 0.50457, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 324s 251ms/step - loss: 0.5046\n",
      "Epoch 38/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.5012\n",
      "Epoch 38: loss improved from 0.50457 to 0.50117, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 327s 253ms/step - loss: 0.5012\n",
      "Epoch 39/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4898\n",
      "Epoch 39: loss improved from 0.50117 to 0.48976, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 325s 252ms/step - loss: 0.4898\n",
      "Epoch 40/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4773\n",
      "Epoch 40: loss improved from 0.48976 to 0.47733, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 0.4773\n",
      "Epoch 41/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4693\n",
      "Epoch 41: loss improved from 0.47733 to 0.46927, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 315s 243ms/step - loss: 0.4693\n",
      "Epoch 42/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4621\n",
      "Epoch 42: loss improved from 0.46927 to 0.46206, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 0.4621\n",
      "Epoch 43/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4578\n",
      "Epoch 43: loss improved from 0.46206 to 0.45781, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 0.4578\n",
      "Epoch 44/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4501\n",
      "Epoch 44: loss improved from 0.45781 to 0.45012, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 326s 252ms/step - loss: 0.4501\n",
      "Epoch 45/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4424\n",
      "Epoch 45: loss improved from 0.45012 to 0.44237, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 0.4424\n",
      "Epoch 46/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4395\n",
      "Epoch 46: loss improved from 0.44237 to 0.43950, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 0.4395\n",
      "Epoch 47/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4311\n",
      "Epoch 47: loss improved from 0.43950 to 0.43106, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 0.4311\n",
      "Epoch 48/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4275\n",
      "Epoch 48: loss improved from 0.43106 to 0.42748, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 321s 248ms/step - loss: 0.4275\n",
      "Epoch 49/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4192\n",
      "Epoch 49: loss improved from 0.42748 to 0.41917, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 322s 249ms/step - loss: 0.4192\n",
      "Epoch 50/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4184\n",
      "Epoch 50: loss improved from 0.41917 to 0.41840, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 0.4184\n",
      "Epoch 51/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4116\n",
      "Epoch 51: loss improved from 0.41840 to 0.41157, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 318s 246ms/step - loss: 0.4116\n",
      "Epoch 52/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4069\n",
      "Epoch 52: loss improved from 0.41157 to 0.40695, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 326s 252ms/step - loss: 0.4069\n",
      "Epoch 53/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.4031\n",
      "Epoch 53: loss improved from 0.40695 to 0.40306, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 0.4031\n",
      "Epoch 54/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3998\n",
      "Epoch 54: loss improved from 0.40306 to 0.39985, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 0.3998\n",
      "Epoch 55/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3992\n",
      "Epoch 55: loss improved from 0.39985 to 0.39917, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 323s 250ms/step - loss: 0.3992\n",
      "Epoch 56/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3950\n",
      "Epoch 56: loss improved from 0.39917 to 0.39502, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 319s 247ms/step - loss: 0.3950\n",
      "Epoch 57/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3913\n",
      "Epoch 57: loss improved from 0.39502 to 0.39128, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 325s 251ms/step - loss: 0.3913\n",
      "Epoch 58/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3877\n",
      "Epoch 58: loss improved from 0.39128 to 0.38766, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 322s 249ms/step - loss: 0.3877\n",
      "Epoch 59/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3813\n",
      "Epoch 59: loss improved from 0.38766 to 0.38129, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 339s 262ms/step - loss: 0.3813\n",
      "Epoch 60/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3809\n",
      "Epoch 60: loss improved from 0.38129 to 0.38095, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 350s 271ms/step - loss: 0.3809\n",
      "Epoch 61/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3760\n",
      "Epoch 61: loss improved from 0.38095 to 0.37599, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 346s 268ms/step - loss: 0.3760\n",
      "Epoch 62/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3770\n",
      "Epoch 62: loss did not improve from 0.37599\n",
      "1293/1293 [==============================] - 346s 268ms/step - loss: 0.3770\n",
      "Epoch 63/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3757\n",
      "Epoch 63: loss improved from 0.37599 to 0.37569, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 349s 270ms/step - loss: 0.3757\n",
      "Epoch 64/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3703\n",
      "Epoch 64: loss improved from 0.37569 to 0.37030, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 347s 268ms/step - loss: 0.3703\n",
      "Epoch 65/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3682\n",
      "Epoch 65: loss improved from 0.37030 to 0.36820, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 349s 270ms/step - loss: 0.3682\n",
      "Epoch 66/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3659\n",
      "Epoch 66: loss improved from 0.36820 to 0.36585, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 347s 269ms/step - loss: 0.3659\n",
      "Epoch 67/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3633\n",
      "Epoch 67: loss improved from 0.36585 to 0.36333, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 348s 269ms/step - loss: 0.3633\n",
      "Epoch 68/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3623\n",
      "Epoch 68: loss improved from 0.36333 to 0.36229, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 349s 270ms/step - loss: 0.3623\n",
      "Epoch 69/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3559\n",
      "Epoch 69: loss improved from 0.36229 to 0.35586, saving model to next_words.h5\n",
      "1293/1293 [==============================] - 344s 266ms/step - loss: 0.3559\n",
      "Epoch 70/70\n",
      "1293/1293 [==============================] - ETA: 0s - loss: 0.3594\n",
      "Epoch 70: loss did not improve from 0.35586\n",
      "1293/1293 [==============================] - 327s 253ms/step - loss: 0.3594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x213e2c0b890>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    " \n",
    "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001),metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=70, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdda35a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 3 words : passed a dreadful\n",
      "1/1 [==============================] - 1s 628ms/step\n",
      "evening\n",
      "Enter 3 words : I cannot mimic\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "one\n",
      "Enter 3 words : exit\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    " \n",
    "model = load_model('next_words.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    " \n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "  seq = tokenizer.texts_to_sequences([text])\n",
    "  seq = np.array(seq)\n",
    "  preds = np.argmax(model.predict(seq))\n",
    "  predicted_word = \"\"\n",
    "   \n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "   \n",
    "  print(predicted_word)\n",
    "\n",
    "    \n",
    "    \n",
    "while(True):\n",
    "    txt = input('Enter 3 words : ')\n",
    "    if(txt=='exit'):\n",
    "        break\n",
    "    Predict_Next_Words(model, tokenizer, txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcd0d3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 3 words : passed a dreadful\n",
      "1/1 [==============================] - 1s 838ms/step\n",
      "evening\n",
      "Enter 3 words : I cannot mimic\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "one\n",
      "Enter 3 words : exit\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    txt = input('Enter 3 words : ')\n",
    "    if(txt=='exit'):\n",
    "        break\n",
    "    Predict_Next_Words(model, tokenizer, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa06d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "model = load_model('next_words.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
